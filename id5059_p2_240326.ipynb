{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id5059_p2_group_project_2024.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install libraries\n",
    "# !{sys.executable} -m pip install numpy pandas matplotlib scikit-learn | grep -v 'already satisfied'#pip-env\n",
    "# %conda install numpy pandas matplotlib scikit-learn# -y#conda-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "pandas version: 2.2.1\n",
      "matplotlib version: 3.8.0\n",
      "scikit-learn version: 1.2.2\n"
     ]
    }
   ],
   "source": [
    "import numpy# as np\n",
    "import pandas# as pd\n",
    "import matplotlib# as plt\n",
    "import sklearn\n",
    "\n",
    "print(\"numpy version:\", numpy.__version__)\n",
    "print(\"pandas version:\", pandas.__version__)\n",
    "print(\"matplotlib version:\", matplotlib.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id5059 Predicting Liver Cirrhosis Outcomes (Group Project 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset from Kaggle (fully labelled synthetic dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the Specification\n",
    "- Our synthetic dataset (for id5059 2024 and Kaggle competition 2023) https://www.kaggle.com/competitions/playground-series-s3e26/\n",
    "- The real dataset (Mayo Clinic 1974 to 1984) https://www.kaggle.com/datasets/joebeachcapital/cirrhosis-patient-survival-prediction\n",
    "#### Additional links\n",
    "- Generic web search (kaggle + liver + cirrhosis) https://duckduckgo.com/?q=kaggle+liver+cirrhosis\n",
    "- Various research papers on ML approaches to this task\n",
    "\n",
    "The reason medics collect data on these markers is because we know these features predict disease outcome already from medical domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our synthetic dataset is based on the real dataset from the Mayo Clinic collected across 1974 to 1984, which has the following description from Kaggle: \n",
    "\n",
    "\n",
    "\"About Dataset\n",
    "\n",
    "Utilize 17 clinical features for predicting survival state of patients with liver cirrhosis. The survival states include 0 = D (death), 1 = C (censored), 2 = CL (censored due to liver transplantation).\n",
    "\n",
    "For what purpose was the dataset created?\n",
    "\n",
    "Cirrhosis results from prolonged liver damage, leading to extensive scarring, often due to conditions like hepatitis or chronic alcohol consumption. The data provided is sourced from a Mayo Clinic study on primary biliary cirrhosis (PBC) of the liver carried out from 1974 to 1984.\n",
    "\n",
    "Who funded the creation of the dataset?\n",
    "\n",
    "Mayo Clinic\n",
    "\n",
    "What do the instances in this dataset represent?\n",
    "\n",
    "People\n",
    "\n",
    "Does the dataset contain data that might be considered sensitive in any way?\n",
    "\n",
    "Sex, Age\n",
    "\n",
    "Was there any data preprocessing performed?\n",
    "\n",
    "    Drop all the rows where miss value (NA) were present in the Drug column\n",
    "    Impute missing values with mean results\n",
    "    One-hot encoding for all category attributes\n",
    "\n",
    "Additional Information\n",
    "\n",
    "During 1974 to 1984, 424 PBC patients referred to the Mayo Clinic qualified for the randomized placebo-controlled trial testing the drug D-penicillamine. Of these, the initial 312 patients took part in the trial and have mostly comprehensive data. The remaining 112 patients didn't join the clinical trial but agreed to record basic metrics and undergo survival tracking. Six of these patients were soon untraceable after their diagnosis, leaving data for 106 of these individuals in addition to the 312 who were part of the randomized trial.\"\n",
    "\n",
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the synthetic dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique `Status` values:\n",
      " ['D' 'C' 'CL'] \n",
      "\n",
      "Counts of each `Status` value in `train.csv`:\n",
      " Status\n",
      "C     4965\n",
      "D     2665\n",
      "CL     275\n",
      "Name: count, dtype: int64 \n",
      " \n",
      "Length of `train.csv`\n",
      " 7905 \n",
      "\n",
      "Count of unknown `Status` values to be predicted across `test.csv`:\n",
      " 5271\n",
      "\n",
      "Total data entries across `train.csv` and `test.csv`:\n",
      " 13176\n",
      "\n",
      "Percentage share of total data entries across `train.csv` and `test.csv`:\n",
      " train.csv 59.99544626593807 %\n",
      " test.csv 40.00455373406193 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('/Users/user/Documents/cscrawl/cs/studres/ID5059/Coursework/Coursework-2/data')\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Data exploration\n",
    "    # How sparse is our data? Quantify its sparseness so we can move on.\n",
    "    # Our data is especially sparse with respect to transplants.\n",
    "\n",
    "# List the unique values in the 'Status' column, ={​​D,C,CL}​​ as expected\n",
    "unique_statuses = df_train['Status'].unique()\n",
    "\n",
    "# Count the occurrences of each value in the 'Status' column\n",
    "status_counts = df_train['Status'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print('Unique `Status` values:\\n',unique_statuses,'\\n')\n",
    "print('Counts of each `Status` value in `train.csv`:\\n',df_train['Status'].value_counts(),'\\n','\\nLength of `train.csv`\\n',len(df_train),'\\n')\n",
    "print('Count of unknown `Status` values to be predicted across `test.csv`:\\n',len(df_test))\n",
    "\n",
    "print('\\nTotal data entries across `train.csv` and `test.csv`:\\n',len(df_train)+len(df_test))\n",
    "print('\\nPercentage share of total data entries across `train.csv` and `test.csv`:\\n',\n",
    "      'train.csv',100*len(df_train)/(len(df_train)+len(df_test)),'%\\n',\n",
    "      'test.csv',100*len(df_test)/(len(df_train)+len(df_test)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7905 entries, 0 to 7904\n",
      "Data columns (total 20 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             7905 non-null   int64  \n",
      " 1   N_Days         7905 non-null   int64  \n",
      " 2   Drug           7905 non-null   object \n",
      " 3   Age            7905 non-null   int64  \n",
      " 4   Sex            7905 non-null   object \n",
      " 5   Ascites        7905 non-null   object \n",
      " 6   Hepatomegaly   7905 non-null   object \n",
      " 7   Spiders        7905 non-null   object \n",
      " 8   Edema          7905 non-null   object \n",
      " 9   Bilirubin      7905 non-null   float64\n",
      " 10  Cholesterol    7905 non-null   float64\n",
      " 11  Albumin        7905 non-null   float64\n",
      " 12  Copper         7905 non-null   float64\n",
      " 13  Alk_Phos       7905 non-null   float64\n",
      " 14  SGOT           7905 non-null   float64\n",
      " 15  Tryglicerides  7905 non-null   float64\n",
      " 16  Platelets      7905 non-null   float64\n",
      " 17  Prothrombin    7905 non-null   float64\n",
      " 18  Stage          7905 non-null   float64\n",
      " 19  Status         7905 non-null   object \n",
      "dtypes: float64(10), int64(3), object(7)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the full training data csv\n",
    "full_data_path = 'train.csv'\n",
    "full_data = pd.read_csv(full_data_path)\n",
    "# Verify basic information like length of the full training data csv\n",
    "full_data_info = full_data.info()\n",
    "# full_data_head = full_data.head()\n",
    "\n",
    "# full_data_info, full_data_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full `train.csv` dataset is loaded. It contains 7,905 entries and 20 features.\n",
    "\n",
    "Next steps:\n",
    "1. **Splitting the Data:** We'll perform an 80/20 split of the `train.csv` data into a training set and a validation set, using stratified sampling to maintain the distribution of the `Status` classes.\n",
    "    \n",
    "2. **Splitting Imbalance Data:** Given the class imbalance, particularly the small proportion of the 'CL' class, we'll apply a stratified sampling technique during the split to avoid distorting the class distributions.\n",
    "    \n",
    "[ToDo] 3. **Deletion Scheme:** To explore the impact of data reduction while respecting the class imbalance, we'll initially remove 10% of the data using a stratified approach and later consider varying this percentage.\n",
    "    \n",
    "4. **Pre-processing:** Ahead of the selected models (e.g. gradient boosting, random forest), we'll apply necessary pre-processing steps, including scaling and normalization. This step will also facilitate more meaningful visualizations and analyses.\n",
    "    \n",
    "[ToDo] 5. **Alternative Feature Encoding:** Based on Pamela's suggestion, we can identify the alternative encoding technique to one-hot encoding she mentioned for use with categorical variables [???].\n",
    "    \n",
    "I'll begin by performing my stratified 80/20 data split. ​​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straitfied 80/20 Split into Train/Validation datasets\n",
    "Stratified split, choosing 20% of each class to construct the validation set, in order to maintain the initial class distributions into both the Train and Validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same exact proportions C,D,CL 62.8%,33.7%,3.5% are maintained upon an 80/20 split as the original count C,D,CL 4965,2665,275 is divisible by 5 into 4:1 ratio train:val sets.\n",
      "Status\n",
      "C     0.628083\n",
      "D     0.337128\n",
      "CL    0.034788\n",
      "Name: proportion, dtype: float64 Status\n",
      "C     0.628083\n",
      "D     0.337128\n",
      "CL    0.034788\n",
      "Name: proportion, dtype: float64 Status\n",
      "C     0.628083\n",
      "D     0.337128\n",
      "CL    0.034788\n",
      "Name: proportion, dtype: float64\n",
      "Status\n",
      "C     4965\n",
      "D     2665\n",
      "CL     275\n",
      "Name: count, dtype: int64 Status\n",
      "C     3972\n",
      "D     2132\n",
      "CL     220\n",
      "Name: count, dtype: int64 Status\n",
      "C     993\n",
      "D     533\n",
      "CL     55\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Splitting data 80/20 with stratification to maintain the class proportions\n",
    "# The style for sklearn use seems to be to import specific functions from the documentation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Spliting full_data into `X` (just the features) and `y` (just the target variable `Status`)\n",
    "X = full_data.drop('Status', axis=1)\n",
    "y = full_data['Status']\n",
    "\n",
    "# Splitting the data while maintaining the distribution of the 'Status' classes\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "# Verifying the stratification by comparing class distributions in the full dataset and splits\n",
    "distribution_full = y.value_counts(normalize=True)\n",
    "distribution_train = y_train.value_counts(normalize=True)\n",
    "distribution_val = y_val.value_counts(normalize=True)\n",
    "\n",
    "# distribution_full, distribution_train, distribution_val\n",
    "print('Same exact proportions C,D,CL 62.8%,33.7%,3.5% are maintained upon an 80/20 split as the original count C,D,CL 4965,2665,275 is divisible by 5 into 4:1 ratio train:val sets.')\n",
    "print(distribution_full, distribution_train, distribution_val)\n",
    "print(y.value_counts(), y_train.value_counts(), y_val.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stratified 80/20 split worked, maintaining the distribution of the `Status` classes across the training and validation sets. The class proportions in both subsets closely match the original dataset:\n",
    "\n",
    "    * C: Cirrhosis - Approximately 62.81%\n",
    "    * D: Death - Approximately 33.71%\n",
    "    * CL: Liver Cancer - Approximately 3.48%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing (Categorical features -> OneHot, Numerical features -> zero mean and unit variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__Age</th>\n",
       "      <th>num__Albumin</th>\n",
       "      <th>num__Alk_Phos</th>\n",
       "      <th>num__Bilirubin</th>\n",
       "      <th>num__Cholesterol</th>\n",
       "      <th>num__Copper</th>\n",
       "      <th>num__N_Days</th>\n",
       "      <th>num__Platelets</th>\n",
       "      <th>num__Prothrombin</th>\n",
       "      <th>num__SGOT</th>\n",
       "      <th>...</th>\n",
       "      <th>cat__Sex_M</th>\n",
       "      <th>cat__Ascites_N</th>\n",
       "      <th>cat__Ascites_Y</th>\n",
       "      <th>cat__Hepatomegaly_N</th>\n",
       "      <th>cat__Hepatomegaly_Y</th>\n",
       "      <th>cat__Spiders_N</th>\n",
       "      <th>cat__Spiders_Y</th>\n",
       "      <th>cat__Edema_N</th>\n",
       "      <th>cat__Edema_S</th>\n",
       "      <th>cat__Edema_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.786334</td>\n",
       "      <td>0.735312</td>\n",
       "      <td>-0.437535</td>\n",
       "      <td>1.212277</td>\n",
       "      <td>-0.552543</td>\n",
       "      <td>3.906874</td>\n",
       "      <td>-0.246369</td>\n",
       "      <td>-1.943384</td>\n",
       "      <td>0.480320</td>\n",
       "      <td>0.914594</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.281587</td>\n",
       "      <td>0.618653</td>\n",
       "      <td>-0.423642</td>\n",
       "      <td>-0.150199</td>\n",
       "      <td>-0.609257</td>\n",
       "      <td>-0.443949</td>\n",
       "      <td>-0.077250</td>\n",
       "      <td>-0.398753</td>\n",
       "      <td>-0.929690</td>\n",
       "      <td>-0.315621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.795451</td>\n",
       "      <td>0.443664</td>\n",
       "      <td>-0.193355</td>\n",
       "      <td>-0.176914</td>\n",
       "      <td>0.566258</td>\n",
       "      <td>-0.193447</td>\n",
       "      <td>0.572859</td>\n",
       "      <td>-0.122103</td>\n",
       "      <td>-0.929690</td>\n",
       "      <td>0.578321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.144781</td>\n",
       "      <td>-1.510372</td>\n",
       "      <td>-0.326932</td>\n",
       "      <td>2.681614</td>\n",
       "      <td>0.117706</td>\n",
       "      <td>0.742639</td>\n",
       "      <td>-1.654788</td>\n",
       "      <td>-0.871364</td>\n",
       "      <td>1.121234</td>\n",
       "      <td>0.687975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.753434</td>\n",
       "      <td>0.939465</td>\n",
       "      <td>-0.066723</td>\n",
       "      <td>-0.444067</td>\n",
       "      <td>1.174638</td>\n",
       "      <td>-0.087973</td>\n",
       "      <td>0.199160</td>\n",
       "      <td>0.062331</td>\n",
       "      <td>-1.186055</td>\n",
       "      <td>0.040494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num__Age  num__Albumin  num__Alk_Phos  num__Bilirubin  num__Cholesterol  \\\n",
       "0  2.786334      0.735312      -0.437535        1.212277         -0.552543   \n",
       "1  0.281587      0.618653      -0.423642       -0.150199         -0.609257   \n",
       "2  0.795451      0.443664      -0.193355       -0.176914          0.566258   \n",
       "3  1.144781     -1.510372      -0.326932        2.681614          0.117706   \n",
       "4 -0.753434      0.939465      -0.066723       -0.444067          1.174638   \n",
       "\n",
       "   num__Copper  num__N_Days  num__Platelets  num__Prothrombin  num__SGOT  ...  \\\n",
       "0     3.906874    -0.246369       -1.943384          0.480320   0.914594  ...   \n",
       "1    -0.443949    -0.077250       -0.398753         -0.929690  -0.315621  ...   \n",
       "2    -0.193447     0.572859       -0.122103         -0.929690   0.578321  ...   \n",
       "3     0.742639    -1.654788       -0.871364          1.121234   0.687975  ...   \n",
       "4    -0.087973     0.199160        0.062331         -1.186055   0.040494  ...   \n",
       "\n",
       "   cat__Sex_M  cat__Ascites_N  cat__Ascites_Y  cat__Hepatomegaly_N  \\\n",
       "0         1.0             0.0             1.0                  0.0   \n",
       "1         0.0             1.0             0.0                  0.0   \n",
       "2         0.0             1.0             0.0                  1.0   \n",
       "3         0.0             1.0             0.0                  0.0   \n",
       "4         0.0             1.0             0.0                  1.0   \n",
       "\n",
       "   cat__Hepatomegaly_Y  cat__Spiders_N  cat__Spiders_Y  cat__Edema_N  \\\n",
       "0                  1.0             0.0             1.0           1.0   \n",
       "1                  1.0             1.0             0.0           1.0   \n",
       "2                  0.0             1.0             0.0           1.0   \n",
       "3                  1.0             0.0             1.0           0.0   \n",
       "4                  0.0             1.0             0.0           1.0   \n",
       "\n",
       "   cat__Edema_S  cat__Edema_Y  \n",
       "0           0.0           0.0  \n",
       "1           0.0           0.0  \n",
       "2           0.0           0.0  \n",
       "3           0.0           1.0  \n",
       "4           0.0           0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pre-processing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Identifying categorical and numerical features\n",
    "categorical_features = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
    "numerical_features = X_train.columns.difference(categorical_features + ['id'])  # Exclude 'id' from features\n",
    "\n",
    "# Creating a column transformer to apply different preprocessing to categorical and numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fitting the preprocessor to the training data and transforming both training and validation sets\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "# Converting processed data back to DataFrame for better readability (optional step)\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed, columns = preprocessor.get_feature_names_out())\n",
    "X_val_processed_df = pd.DataFrame(X_val_processed, columns = preprocessor.get_feature_names_out())\n",
    "\n",
    "X_train_processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing steps have been successfully applied:\n",
    "\n",
    "1. **Numerical Features:** Standardized to have zero mean and unit variance.\n",
    "2. **Categorical Features:** One-hot encoded to ensure proper representation without implying ordinal relationships.\n",
    "\n",
    "The transformed training data now has 25 features, because the one-hot encoding expanded the categorical variables into multiple binary features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models (Gradient Boost, Random Forest, ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_val:\n",
      " 0.8266919671094244 \n",
      "\n",
      " classification_report_val:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           C       0.84      0.91      0.88       993\n",
      "          CL       0.62      0.18      0.28        55\n",
      "           D       0.79      0.74      0.77       533\n",
      "\n",
      "    accuracy                           0.83      1581\n",
      "   macro avg       0.75      0.61      0.64      1581\n",
      "weighted avg       0.82      0.83      0.82      1581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)#random_seed=42\n",
    "\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = gb_classifier.predict(X_val_processed)\n",
    "\n",
    "# Calculate accuracy and generate a classification report\n",
    "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "classification_report_val = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# accuracy_val, classification_report_val\n",
    "print('accuracy_val:\\n',accuracy_val,'\\n\\n','classification_report_val:\\n', classification_report_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient boosting classifier achieved an accuracy of approximately 82.67% on the validation set.\n",
    "\n",
    "Breakdown of the model's performance:\n",
    "\n",
    "* **Precision:**\n",
    "    * Class C (Cirrhosis): 84%\n",
    "    * Class CL (Transplant): 62%\n",
    "    * Class D (Death): 79%\n",
    "* **Recall:**\n",
    "    * Class C (Cirrhosis): 91%\n",
    "    * Class CL (Transplant): 18%\n",
    "    * Class D (Death): 74%\n",
    "* **F1-Score:**\n",
    "    * Class C (Cirrhosis): 0.88\n",
    "    * Class CL (Transplant): 0.28\n",
    "    * Class D (Death): 0.77\n",
    "\n",
    "The model performs well for the C and D classes (84% and 79%), but struggles with the CL class (62%), likely due to CL's smaller presence in the dataset. This is a weakness of the model that should be improved by exploiting techniques to address the class imbalance more effectively and/or by exploring more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible next steps:\n",
    "\n",
    "    1. Try more sophisticated models or ensemble techniques.\n",
    "    2. Techniques to handle imbalanced data, like SMOTE or Class Weights Adjustment.\n",
    "    3. Hyperparameter tuning of the gradient boosting model.\n",
    "    4. Comparing these results with a neural network model, considering its potential if we could achieve appropriate scaling and normalization, although I expect we will not achieve that to a high enough standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_val_rf:\n",
      " 0.8298545224541429 \n",
      "\n",
      " classification_report_val_rf:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           C       0.85      0.91      0.88       993\n",
      "          CL       0.80      0.07      0.13        55\n",
      "           D       0.80      0.75      0.77       533\n",
      "\n",
      "    accuracy                           0.83      1581\n",
      "   macro avg       0.81      0.58      0.59      1581\n",
      "weighted avg       0.83      0.83      0.82      1581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_rf = rf_classifier.predict(X_val_processed)\n",
    "\n",
    "# Calculate accuracy and generate a classification report for Random Forest\n",
    "accuracy_val_rf = accuracy_score(y_val, y_val_pred_rf)\n",
    "classification_report_val_rf = classification_report(y_val, y_val_pred_rf)\n",
    "\n",
    "# accuracy_val_rf, classification_report_val_rf\n",
    "print('accuracy_val_rf:\\n',accuracy_val_rf,'\\n\\n','classification_report_val_rf:\\n', classification_report_val_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest classifier achieved an accuracy of approximately 82.99% on the validation set, which is slightly better than the Gradient Boosting model.\n",
    "\n",
    "Performance breakdown for the Random Forest classifier:\n",
    "\n",
    "* **Precision:**\n",
    "    *   Class C (Cirrhosis): 85%\n",
    "    *   Class CL (Transplant): 80% #Suspiciously good, so have I done it wrong?\n",
    "    *   Class D (Death): 80%\n",
    "* **Recall:**\n",
    "    *   Class C (Cirrhosis): 91%\n",
    "    *   Class CL (Transplant): 7%\n",
    "    *   Class D (Death): 75%\n",
    "* **F1-Score:**\n",
    "    *   Class C (Cirrhosis): 0.88\n",
    "    *   Class CL (Transplant): 0.13\n",
    "    *   Class D (Death): 0.77\n",
    "\n",
    "Precision for the CL class improved significantly with the Random Forest model c/w Gradient Boost (80% versus 62%). But Random Forest's recall remains very low (7%), and worse than Gradient Boost (18%), indicating that the model struggles to correctly identify the CL class instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Techniques (SMOTE and Class Weights Adjustment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance via SMOTE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address Class Imbalance with SMOTE\n",
    "\n",
    "To address the low recall for the CL class, I will apply the Synthetic Minority Over-sampling Technique (SMOTE). This technique generates synthetic samples in the feature space for the minority class, which can help improve classifier performance on imbalanced datasets.\n",
    "\n",
    "I'll apply SMOTE to the training data and then retrain the Random Forest classifier to observe any improvements in handling the CL class:\n",
    "\n",
    "1. Apply SMOTE to the training data.\n",
    "2. Retrain the Random Forest classifier using the SMOTE-enhanced data.\n",
    "3. Evaluate the model performance on the original (non-SMOTE) validation set. ​​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_val_rf_smote:\n",
      " 0.8102466793168881 \n",
      "\n",
      " classification_report_val_rf_smote:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           C       0.87      0.86      0.86       993\n",
      "          CL       0.34      0.29      0.31        55\n",
      "           D       0.75      0.77      0.76       533\n",
      "\n",
      "    accuracy                           0.81      1581\n",
      "   macro avg       0.65      0.64      0.65      1581\n",
      "weighted avg       0.81      0.81      0.81      1581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try imblearn library for SMOTE\n",
    "    # search: https://duckduckgo.com/?q=imblearn+smote\n",
    "    # documentation: https://imbalanced-learn.org/dev/over_sampling.html#smote-adasyn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "# Retrain the classifier using the SMOTE-enhanced data\n",
    "rf_classifier_smote = RandomForestClassifier(random_state=42)\n",
    "rf_classifier_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_rf_smote = rf_classifier_smote.predict(X_val_processed)\n",
    "\n",
    "# Calculate accuracy and generate a classification report for Random Forest after SMOTE\n",
    "accuracy_val_rf_smote = accuracy_score(y_val, y_val_pred_rf_smote)\n",
    "classification_report_val_rf_smote = classification_report(y_val, y_val_pred_rf_smote)\n",
    "\n",
    "# accuracy_val_rf_smote, classification_report_val_rf_smote\n",
    "print('accuracy_val_rf_smote:\\n',accuracy_val_rf_smote,'\\n\\n','classification_report_val_rf_smote:\\n', classification_report_val_rf_smote)\n",
    "# # Compare with the RandomForest pre-SMOTE\n",
    "# print('original accuracy_val_rf:\\n',accuracy_val_rf,'\\n\\n','original classification_report_val_rf:\\n', classification_report_val_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding SMOTE to RandomForest trades some overall accuracy (81.02% down from 82.99%) in exchange for improved CL recall (29% up from 7%) and f1-score (0.31 up from 0.13).\n",
    "\n",
    "Note also that Precision has been revised downwards, including for CL (34% down from 80%), but I do not believe the previous RandomForest CL Precision figure of 80%.\n",
    "\n",
    "Breakdown of the SMOTE + RandomForest model performance:\n",
    "\n",
    "* **Precision:**\n",
    "    *   Class C (Cirrhosis): 87% up from 85%\n",
    "    *   Class CL (Transplant): 34% down from 80% #Suspicious\n",
    "    *   Class D (Death): 75% down from 80%\n",
    "* **Recall:**\n",
    "    *   Class C (Cirrhosis): 86% down from 91%\n",
    "    *   Class CL (Transplant): 29% up from 7%\n",
    "    *   Class D (Death): 77% up from 75%\n",
    "* **F1-Score:**\n",
    "    *   Class C (Cirrhosis): 0.86 down from 0.88\n",
    "    *   Class CL (Transplant): 0.31 up from 0.13\n",
    "    *   Class D (Death): 0.76 down from 0.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of SMOTE results:\n",
    "\n",
    "* **Accuracy cost:** \n",
    "Slight decrease in overall accuracy from 82.99% to 81.02%. This decrease could be attributed to the model now paying more attention to the minority class (CL) at the expense of overall accuracy.\n",
    "\n",
    "* **Improved Recall for CL:** \n",
    "The recall for the CL class has improved from 7% to 29%. This indicates that the SMOTE-enhanced model is now better at identifying positive instances of the CL class compared to the original model. Recall is crucial in medical contexts like this, as missing out on true positive cases can be more detrimental than false positives.\n",
    "Precision measure for CL apparently decreases: Precision drops for the CL class from 80% to 34%. The pre-SMOTE 80% figure may be misleading because it was achieved by correctly predicting true negatives across the dataset because most data entries are negative (i.e. not CL). Now our precision measure for CL is lower because it predicts more false positives (a penalized error), in order to also predict more true positive (clinically more important).\n",
    "\n",
    "* **F1-Score Increase for CL:** \n",
    "The F1-score balances precision and recall, and has improved from 0.13 to 0.31 for the CL class, underscoring the trade-off between recall and precision post-SMOTE is actually a favorable improvement for CL with more balanced performance.\n",
    "\n",
    "* **Performance on Other Classes:** \n",
    "We set out to improve performance on the CL class because our data is imbalanced, but this has come at the expense of the classifier's performance across the other classes D and C, which may be either desirable or undesirable for our clinical use case. Class imbalance techniques lead to redistribution of the classifier's focus across the classes, and this has costs.\n",
    "\n",
    "**Summary:**\n",
    "Applying SMOTE has made our Random Forest model more sensitive to the minority class (CL). This is evident from the improved recall, and may be particularly desirable in our medical use case where detecting positive cases is critical, even at the expense of more false positive predictions. But this came at the cost of precision, per the typical trade-off between recall and precision in imbalanced datasets. An improved F1-score for the CL class confirms that SMOTE has been effective in enhancing the model's ability to classify this minority class more accurately, despite the slight decrease in overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance via Class Weights Adjustment instead of SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_val_rf_weighted:\n",
      " 0.8298545224541429 \n",
      "\n",
      " classification_report_val_rf_weighted:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           C       0.84      0.92      0.88       993\n",
      "          CL       0.86      0.11      0.19        55\n",
      "           D       0.80      0.74      0.77       533\n",
      "\n",
      "    accuracy                           0.83      1581\n",
      "   macro avg       0.83      0.59      0.61      1581\n",
      "weighted avg       0.83      0.83      0.82      1581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adjusting class weights for the Random Forest classifier\n",
    "class_weights = {\n",
    "    \"C\": 1,\n",
    "    \"D\": y_train.value_counts()[\"C\"] / y_train.value_counts()[\"D\"],\n",
    "    \"CL\": y_train.value_counts()[\"C\"] / y_train.value_counts()[\"CL\"]\n",
    "}\n",
    "# C is assigned a baseline weight = 1, note that this does not prioritize the C class over the others\n",
    "# CL is a minority class, so its ratio is expected to be large, significantly increasing the penalty for misclassifying CL class instances compared to C or D\n",
    "\n",
    "rf_classifier_weighted = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
    "\n",
    "# Retrain the classifier using the training data with adjusted class weights\n",
    "rf_classifier_weighted.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_rf_weighted = rf_classifier_weighted.predict(X_val_processed)\n",
    "\n",
    "# Calculate accuracy and generate a classification report for the weighted Random Forest\n",
    "accuracy_val_rf_weighted = accuracy_score(y_val, y_val_pred_rf_weighted)\n",
    "classification_report_val_rf_weighted = classification_report(y_val, y_val_pred_rf_weighted)\n",
    "\n",
    "# accuracy_val_rf_weighted, classification_report_val_rf_weighted\n",
    "print('accuracy_val_rf_weighted:\\n',accuracy_val_rf_weighted,'\\n\\n','classification_report_val_rf_weighted:\\n', classification_report_val_rf_weighted)\n",
    "# # Compare with the RandomForest pre-SMOTE\n",
    "# print('original accuracy_val_rf:\\n',accuracy_val_rf,'\\n\\n','original classification_report_val_rf:\\n', classification_report_val_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Weights Adjustment influence on RandomForest model performance:\n",
    "\n",
    "* **Precision:**\n",
    "    *   Class C (Cirrhosis): 84% down from 85% (versus 87% SMOTE)\n",
    "    *   Class CL (Transplant): 86% up from 80% (versus 34% SMOTE)\n",
    "    *   Class D (Death): 80% same as 80% (versus 75% SMOTE)\n",
    "* **Recall:**\n",
    "    *   Class C (Cirrhosis): 92% up from 91% (versus 86% SMOTE)\n",
    "    *   Class CL (Transplant): 11% up from 7% (versus 29% SMOTE)\n",
    "    *   Class D (Death): 74% down from 75% (versus 77% SMOTE)\n",
    "* **F1-Score:**\n",
    "    *   Class C (Cirrhosis): 0.88 same as 0.88 (versus 0.86 SMOTE)\n",
    "    *   Class CL (Transplant): 0.19 up from 0.13 (versus 0.31 SMOTE)\n",
    "    *   Class D (Death): 0.77 same as 0.77 (versus 0.76 SMOTE)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, Class Weights Adjustment is less effective than SMOTE at addressing class imbalance with CL F1-Scores of 0.19 versus 0.31 for SMOTE and a 0.13 baseline. Other implementations may vary; this implementation is simple ratio scaling of each of D and CL with respect to C, the most prevalent class. SMOTE also cost slightly more in terms of overall model performance with respect to C and D. So less effectively addressing class imbalance through Class Weights Adjustment also better preserved model performance with respect to C and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
